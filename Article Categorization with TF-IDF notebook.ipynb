{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SinaDadvand_Project 1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoWQAC2JJoZQ"
      },
      "source": [
        "# **Article Categorization with TF-IDF Score**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RSQxaddASq-"
      },
      "source": [
        "Text classification and article categorization are some of the basic use cases in text analytics. In the project presented in this notebook, we are going to define a custom function to rate text documents based on TF-IDF score. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2yTa45FJzAW"
      },
      "source": [
        "**Import Libraries** \n",
        "\n",
        "As usual, I would like to start my Python environment with importing the proper libraries that will be utilized later on.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDUhE_YeJp9_"
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import sqlite3\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from math import log\n",
        "\n",
        "import dfply"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6W0GVzgJnaT"
      },
      "source": [
        "**Loading the Data**   \n",
        "The data is for this project is provided as a SQLike database. This databse contains only one Article table with three columns: \n",
        "\n",
        "\n",
        "*   **id** - the primary key of the table, the unique identifier for each article\n",
        "*   **category** -  column consisting of predefined article categories, the label\n",
        "*   **raw_text** - the actual text of each article\n",
        "\n",
        "In order to load this data in, we are going to use sqlite3.connect function and query all records from the table to a pandas dataframe.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CyRk6e3Jpmc",
        "outputId": "56fb505e-db68-4f16-a492-b43cc020e1fb"
      },
      "source": [
        "conn = sqlite3.connect('Project 01 - Database.db')\n",
        "sql = 'SELECT * FROM Article'\n",
        "df = pd.read_sql_query(sql, conn, index_col='id')\n",
        "conn.close()\n",
        "#------------------------------\n",
        "df.head(10)\n",
        "df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2225, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIQT1EWwLScH"
      },
      "source": [
        "**Cleaning the Data** \n",
        "\n",
        "As first step for our analysis, we are going to prepare our data by cleaning the raw text of each article. This involves multiple steps such as removing white space, underscores, etc. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8x1YnUFLcnF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "7ceda2a7-37f4-41a6-b71b-576b2e25943e"
      },
      "source": [
        "def clean_text(raw_text):\n",
        "  #convert the raw text to lowercase\n",
        "  text = raw_text.lower()\n",
        "  #remove all numbers from the text using a regular expression\n",
        "  text = re.sub(r'[0-9]', ' ', text)\n",
        "  #remove all underscores from the text\n",
        "  text = re.sub(r'\\_', ' ', text)\n",
        "  #remove anything else in the text that isn't a word character or a space (e.g., punctuation, special symbols, etc.)\n",
        "  text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "  #remove any excess whitespace\n",
        "  for _ in range(10):\n",
        "    text = text.replace('  ', ' ')\n",
        "  #remove any leading or trailing space characters\n",
        "  text = text.strip()\n",
        "  #return the clean text\n",
        "  return text\n",
        "\n",
        "#------------------------------------\n",
        "df['clean_text'] = [clean_text(raw_text) for raw_text in df.raw_text]\n",
        "\n",
        "df.head(4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>category</th>\n",
              "      <th>raw_text</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>6347</th>\n",
              "      <td>Politics</td>\n",
              "      <td>Hiding women away in the home hidden behind ve...</td>\n",
              "      <td>hiding women away in the home hidden behind ve...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13840</th>\n",
              "      <td>Sports</td>\n",
              "      <td>Celtic brushed aside Clyde to secure their pla...</td>\n",
              "      <td>celtic brushed aside clyde to secure their pla...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14775</th>\n",
              "      <td>Unknown</td>\n",
              "      <td>If you have finished Doom 3, Half Life 2 and H...</td>\n",
              "      <td>if you have finished doom half life and halo d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16641</th>\n",
              "      <td>Unknown</td>\n",
              "      <td>Controversial new UK casinos will be banned fr...</td>\n",
              "      <td>controversial new uk casinos will be banned fr...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       category  ...                                         clean_text\n",
              "id               ...                                                   \n",
              "6347   Politics  ...  hiding women away in the home hidden behind ve...\n",
              "13840    Sports  ...  celtic brushed aside clyde to secure their pla...\n",
              "14775   Unknown  ...  if you have finished doom half life and halo d...\n",
              "16641   Unknown  ...  controversial new uk casinos will be banned fr...\n",
              "\n",
              "[4 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAiXLa84Oi4I"
      },
      "source": [
        "**Building Vocabulary** \n",
        "\n",
        "Next, we will combine the text from all articles and create a vocabulary of words and their frequencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNuSV8MnOahz"
      },
      "source": [
        "#build a vocabulary of words\n",
        "all_text = ' '.join(df.clean_text) #join all of the English texts into one big string\n",
        "words = all_text.split() #split the text into words\n",
        "word_frequencies = Counter(words) #count all words in the text\n",
        "vocabulary = list(word_frequencies.keys()) #get a list of all unique words\n",
        "\n",
        "len(vocabulary)\n",
        "\n",
        "vocabulary.sort()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfaqDypdIWIC"
      },
      "source": [
        "# **Calculating TF-IDF**\n",
        "TF-IDF score here, is calculated for each word that appears in each document as the Term Frequency multipled by Inverse Document Frequency.\n",
        "\n",
        "Term Frequncy - number of the time a word appears in a document / total number of words in that document.\n",
        "\n",
        "Inverse Document Frequency - log( total number of documents in the corpus / number of documents in the corpus containing that word )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wtq4vtgiRghO"
      },
      "source": [
        "**Computing Inverse Document Frequency (IDF)**\n",
        "\n",
        "First, we will go over our entities in the created vocabulary and calculate the IDF for each word.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "2eUqz49RRmlm",
        "outputId": "9ad35700-7a05-4361-bb63-970a3717926d"
      },
      "source": [
        "vocab = pd.DataFrame (vocabulary,columns=['words'])\n",
        "vocab[\"IDF\"] = \"\"\n",
        "N = df.shape[0]  # Total Number of Documents in the Corpus\n",
        "for word in vocab.words:\n",
        "  Nw = 0\n",
        "  for row in df.itertuples():\n",
        "    if word in row.clean_text:\n",
        "      Nw += 1\n",
        "  idf = log(N/Nw)  \n",
        "  vocab.loc[vocab.words==word, 'IDF'] = idf\n",
        "\n",
        "#------------------- Checking the length of Vocab and IDF\n",
        "vocab.head(5)\n",
        "\n",
        "# Code for checking IDF for a word\n",
        "# vocab.loc[vocab.words==\"the\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>words</th>\n",
              "      <th>IDF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>aa</td>\n",
              "      <td>3.10234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>aaa</td>\n",
              "      <td>4.99946</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>aaas</td>\n",
              "      <td>5.7616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>aac</td>\n",
              "      <td>6.6089</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  words      IDF\n",
              "0     a        0\n",
              "1    aa  3.10234\n",
              "2   aaa  4.99946\n",
              "3  aaas   5.7616\n",
              "4   aac   6.6089"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v04RjWHRl2u"
      },
      "source": [
        "**Defining an Article Class**\n",
        "\n",
        "Here, we are utilizing the class definition in Python in order to store the atributes related to each article in a more tidy and accessible form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaRnm5C6gjG-"
      },
      "source": [
        "class Article:\n",
        "  def __init__(self, document_id, category, Term_Freq, total_words, TF_IDF, estimated_topic):\n",
        "    self.id = document_id #the document's unique ID number\n",
        "    self.category = category #the document's topic\n",
        "    self.total_words = total_words #the total number of words in the document\n",
        "    self.Term_Freq = Term_Freq \n",
        "    self.TF_IDF = TF_IDF\n",
        "    self.estimated_topic = estimated_topic\n",
        "    # self.word_probabilities = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1hQXStfgdJE"
      },
      "source": [
        "**Calculating Term Frequency for each Article**\n",
        "\n",
        "Next, we will go over every single article in our dataframe, and compute the Term Frequencies for each word appearing in that article."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PkMry0kg8MZ"
      },
      "source": [
        "articles= []\n",
        "\n",
        "for row in df.itertuples():\n",
        "  words = row.clean_text.split()\n",
        "  article_word_freq = Counter(words)\n",
        "  Nd = sum(article_word_freq.values())\n",
        "  TF = []\n",
        "  for vocab_word in vocab.words:\n",
        "    Fwd = 0\n",
        "    if vocab_word in article_word_freq:\n",
        "      Fwd = article_word_freq[vocab_word]\n",
        "    term_freq = Fwd / Nd\n",
        "    TF.append(term_freq) \n",
        "\n",
        "  articles.append(Article(row.Index, row.category, TF, Nd, 0, \"\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFzxcGJGg8zN"
      },
      "source": [
        "**Computing TF-IDF**\n",
        "\n",
        "Now that we have IDF values for our vocabulary, and the Term Frequency related to each article, we can calculate the TF-IDF score for each article."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOkCYm7ig9Pp"
      },
      "source": [
        "for article in articles:\n",
        "  article.TF_IDF = article.Term_Freq * np.array(vocab.IDF)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GdgB2wbg9VI"
      },
      "source": [
        "**Computing Average TF_IDF**\n",
        "\n",
        "To be able to classify the category of a new article, we need to define each category as a vector that represents the average TF-IDF score of all articles from that category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ux2O24YKg9eh"
      },
      "source": [
        "def Average_TF_IDF(category):\n",
        "  ave_df = pd.DataFrame()\n",
        "  for article in articles:\n",
        "    if article.category == category:\n",
        "      ave_df[str(article.id)] = article.TF_IDF\n",
        "  ave_df['Mean'] = ave_df.mean(axis=1) \n",
        "  return ave_df.Mean\n",
        "\n",
        "\n",
        "topics_TFIDF = {'Business': np.zeros(len(vocabulary)), 'Sports': np.zeros(len(vocabulary)), 'Politics': np.zeros(len(vocabulary)), 'Technology': np.zeros(len(vocabulary)), 'Entertainment': np.zeros(len(vocabulary))}\n",
        "\n",
        "for topic in topics_TFIDF:\n",
        "  topics_TFIDF[topic] = Average_TF_IDF(category = topic)\n",
        "\n",
        "\n",
        "# Business_TFIDF = Average_TF_IDF(category='Business')\n",
        "# Politics_TFIDF = Average_TF_IDF(category='Politics')\n",
        "# Sports_TFIDF = Average_TF_IDF(category='Sports')\n",
        "# Technology_TFIDF = Average_TF_IDF(category='Technology')\n",
        "# Entertainment_TFIDF = Average_TF_IDF(category='Entertainment')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbx036U7g9j6"
      },
      "source": [
        "**Computing Distance**\n",
        "\n",
        "Let us define a simple function to get the Euclidean distance between an two TF-IDF vectors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOfG_Yk3g6J2"
      },
      "source": [
        "def get_distance(point1, point2):\n",
        "  return np.sqrt(np.sum(np.square(point1 - point2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll9iF0kC401-"
      },
      "source": [
        "**Estimating Unknown Topics**\n",
        "\n",
        "Now, for each article with a category defined as ***Unknown***, we can compute the distance between the article's TF-IDF vector and each of the defined categories average TF-IDF vector.\n",
        "\n",
        "Finally, the estimated category (topic) of the unknown artilce can be deicided as the category with the least distance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yLKTsx841kk"
      },
      "source": [
        "for article in articles:\n",
        "  if article.category == 'Unknown':\n",
        "    distance_from_topics = {'Business':0, 'Politics':0, 'Sports':0 , 'Technology':0 ,'Entertainment':0 }\n",
        "    for topic in ['Business', 'Politics', 'Sports' , 'Technology' ,'Entertainment' ]:\n",
        "      distance_from_topics[topic] = get_distance(article.TF_IDF, topics_TFIDF[topic] )\n",
        "    article.estimated_topic = min(distance_from_topics, key= distance_from_topics.get)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLJ4I1cG42AI"
      },
      "source": [
        "**Creating a Dataframe with Estimates**\n",
        "\n",
        "Finally, we are going to create a dataframe with the estimated categories of the unkown articles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiPTTfVTx4sa"
      },
      "source": [
        "Estimates = pd.DataFrame({'Article_Id':[],'Article_Category':[]})\n",
        "for article in articles:\n",
        "  if article.category == 'Unknown':\n",
        "    Estimates=Estimates.append(pd.DataFrame({'Article_Id':[article.id],'Article_Category':article.estimated_topic} ))\n",
        "    \n",
        " Estimates.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-N_YiLkg5Mn"
      },
      "source": [
        "**Saving CSV File**\n",
        "\n",
        "We can also save the results as a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8V6Gnub_wZy"
      },
      "source": [
        "with open('Dadvand_Kouhi, Sina.csv', 'w') as csvfile:\n",
        "  for row in Estimates.itertuples():\n",
        "    csvfile.write('{},{}\\n'.format(int(row.Article_Id), row.Article_Category))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bV1y5wr8_uCO"
      },
      "source": [
        "**Testing the Process on known categories**\n",
        "\n",
        "Considering a real-world senario in which we do not have access to the real categories of unknown articles, we can test the accuracy of the method on the articles with known category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CevulNh_t1P"
      },
      "source": [
        "Validation_dataframe = pd.DataFrame({'Article_Id':[],'Article_Category':[], 'Estimated_Category':[]})\n",
        "for article in articles:\n",
        "  if article.category != 'Unknown':\n",
        "    distance_from_topics = {'Business':0, 'Politics':0, 'Sports':0 , 'Technology':0 ,'Entertainment':0 }\n",
        "    for topic in ['Business', 'Politics', 'Sports' , 'Technology' ,'Entertainment' ]:\n",
        "      distance_from_topics[topic] = get_distance(article.TF_IDF, topics_TFIDF[topic] )\n",
        "    article.estimated_topic = min(distance_from_topics, key= distance_from_topics.get)\n",
        "    Validation_dataframe=Validation_dataframe.append(pd.DataFrame({'Article_Id':[article.id],'Article_Category':article.category,'Estimated_Category':article.estimated_topic} ))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qpCwC2jc69d"
      },
      "source": [
        "**Condusion Matrix and Accuracy**\n",
        "\n",
        "Finally, we can use the metrics function from sklearn to get a sense of accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jsIuoSGec6jG",
        "outputId": "27930acb-c8d4-4ce0-d681-ac38e2aac1da"
      },
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.confusion_matrix(Validation_dataframe.Article_Category, Validation_dataframe.Estimated_Category, labels=['Business', 'Politics', 'Sports' , 'Technology' ,'Entertainment' ]))\n",
        "\n",
        "print(metrics.classification_report(Validation_dataframe.Article_Category, Validation_dataframe.Estimated_Category, labels=['Business', 'Politics', 'Sports' , 'Technology' ,'Entertainment' ]))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[259   6   0   5   0]\n",
            " [  0 238   0   1   0]\n",
            " [  0   0 294   0   0]\n",
            " [  0   0   0 222   3]\n",
            " [  1   1   0   0 195]]\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "     Business       1.00      0.96      0.98       270\n",
            "     Politics       0.97      1.00      0.98       239\n",
            "       Sports       1.00      1.00      1.00       294\n",
            "   Technology       0.97      0.99      0.98       225\n",
            "Entertainment       0.98      0.99      0.99       197\n",
            "\n",
            "     accuracy                           0.99      1225\n",
            "    macro avg       0.99      0.99      0.99      1225\n",
            " weighted avg       0.99      0.99      0.99      1225\n",
            "\n"
          ]
        }
      ]
    }
  ]
}